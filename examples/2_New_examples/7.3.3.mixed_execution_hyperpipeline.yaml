abstract: |
  This DAG represents a “hyperpipeline” workflow — a difficult-level DAG with multiple levels of branching, cross-branch transformations, wide fan-out and multi-step merging. It is designed to push the orchestrator to its limits in
  terms of:

    - Concurrency
    - File I/O across many transformations
    - Mixed Python/Bash workloads
    - Deep and wide dependency structures
    - Multi-level fan-in / fan-out synchronization

  The pipeline:
    - Generates a dataset
    - Fans out into four primary branches (A–D), each performing two transformations
    - Produces intermediate files consumed by a second-level processing wave
    - Recombines all results in multiple merge stages
    - Ends with a final summary message

  This DAG is suitable for large-scale stress testing.

dag:
  name: "mixed_execution_hyperpipeline"

  tasks:

    # ---------------------------------------------------------
    # 1) INITIALIZATION
    # ---------------------------------------------------------
    - task_id: "init"
      type: "PrintTask"
      params:
        message: "Starting the hyperpipeline..."
      dependencies: []

    # ---------------------------------------------------------
    # 2) DATA GENERATION (Python)
    # ---------------------------------------------------------
    - task_id: "generate_data"
      type: "PythonTask"
      params:
        code: |
          import json, random
          data = {"values": [random.randint(1, 500) for _ in range(30)]}
          with open("hyper_data.json", "w") as f:
            json.dump(data, f)
          print("Generated hyper_data.json with 30 random values.")
      dependencies: ["init"]

    # =========================================================
    # PRIMARY FAN-OUT — 4 BRANCHES A, B, C, D
    # Each branch performs two transformation steps.
    # =========================================================

    # ------------------------ BRANCH A ------------------------
    - task_id: "A1_extract_digits"
      type: "BashTask"
      params:
        command: "grep -o '[0-9]' hyper_data.json > A_digits.txt"
      dependencies: ["generate_data"]

    - task_id: "A2_count_digits"
      type: "BashTask"
      params:
        command: "wc -l A_digits.txt"
      dependencies: ["A1_extract_digits"]

    # ------------------------ BRANCH B ------------------------
    - task_id: "B1_load_and_double"
      type: "PythonTask"
      params:
        code: |
          import json
          with open("hyper_data.json") as f:
            data = json.load(f)
          doubled = [v*2 for v in data["values"]]
          with open("B_doubled.txt", "w") as out:
            out.write("\n".join(map(str, doubled)))
          print("Branch B doubled data.")
      dependencies: ["generate_data"]

    - task_id: "B2_show_head"
      type: "BashTask"
      params:
        command: "head -n 5 B_doubled.txt"
      dependencies: ["B1_load_and_double"]

    # ------------------------ BRANCH C ------------------------
    - task_id: "C1_compute_stats"
      type: "PythonTask"
      params:
        code: |
          import json
          with open("hyper_data.json") as f:
            data = json.load(f)
          vals = data["values"]
          stats = {
            "avg": sum(vals)/len(vals),
            "min": min(vals),
            "max": max(vals)
          }
          with open("C_stats.json", "w") as out:
            json.dump(stats, out)
          print("Branch C computed stats.")
      dependencies: ["generate_data"]

    - task_id: "C2_format_stats"
      type: "BashTask"
      params:
        command: "cat C_stats.json | tr -d '{}\"' | tr ',' '\\n' > C_stats_formatted.txt"
      dependencies: ["C1_compute_stats"]

    # ------------------------ BRANCH D ------------------------
    - task_id: "D1_extract_even"
      type: "PythonTask"
      params:
        code: |
          import json
          with open("hyper_data.json") as f:
            data = json.load(f)
          evens = [v for v in data["values"] if v % 2 == 0]
          with open("D_evens.txt", "w") as out:
            out.write("\n".join(map(str, evens)))
          print("Branch D extracted even numbers.")
      dependencies: ["generate_data"]

    - task_id: "D2_count_even"
      type: "BashTask"
      params:
        command: "wc -l D_evens.txt"
      dependencies: ["D1_extract_even"]

    # =========================================================
    # SECONDARY FAN-IN — merge results from A, B, C, D
    # =========================================================
    - task_id: "merge_primary"
      type: "PrintTask"
      params:
        message: "Primary transformations complete. Launching second-stage processing..."
      dependencies:
        - "A2_count_digits"
        - "B2_show_head"
        - "C2_format_stats"
        - "D2_count_even"

    # =========================================================
    # SECONDARY FAN-OUT — two parallel heavy processing steps
    # =========================================================
    - task_id: "S1_consolidate_text"
      type: "BashTask"
      params:
        command: "cat A_digits.txt B_doubled.txt D_evens.txt 2>/dev/null | wc -l"
      dependencies: ["merge_primary"]

    - task_id: "S2_summary_python"
      type: "PythonTask"
      params:
        code: |
          print('Second-stage summary: Python confirming prior merges completed.')
      dependencies: ["merge_primary"]

    # =========================================================
    # FINAL MERGE
    # =========================================================
    - task_id: "final_merge"
      type: "PrintTask"
      params:
        message: "Second-stage processing complete. Finalizing hyperpipeline..."
      dependencies:
        - "S1_consolidate_text"
        - "S2_summary_python"

    # ---------------------------------------------------------
    # FINAL SUMMARY
    # ---------------------------------------------------------
    - task_id: "finish"
      type: "PrintTask"
      params:
        message: "Hyperpipeline completed successfully!"
      dependencies: ["final_merge"]
